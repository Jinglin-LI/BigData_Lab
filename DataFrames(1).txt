//------------------------------ Car Mileage dataset ------------------------------------------- #
// Download the car mileage dataset from http://www.utdallas.edu/~axn112530/cs6350/dflab/car-milage.csv

// read in the file

val filePath = "/FileStore/tables/FILL_IN_YOUR_PATH/car_milage-6f50d.csv"

// another way to do the above:
//import sys.process._ 
// "wget -P /tmp http://www.utdallas.edu/~axn112530/cs6350/dflab/car-milage.csv" !!
// val filePath = "file:/tmp/car-milage.csv"


val cars = spark.read.option("header","true"). option("inferSchema","true").csv(filePath)

// How many cars are there?
println("Cars has "+<FILL_IN>+" rows") 

// Show the top 5 cars in the dataset


// Print the schema of the dataset


// Let's run some statistical functions

// Find summary of the following columns - mpg,hp,weight,automatic
cars.<FILL_IN>("mpg","hp","weight","automatic").<FILL_IN>

// Run the following query - What is the average mpg and torque of the automatic and non-automatic cars?
cars.<FILL_IN>("automatic").<FILL_IN>("mpg","torque").show()

// Find the overall average mpg and torque of all the cars together
cars.<FILL_IN>(<FILL_IN>(cars("mpg")), <FILL_IN>(cars("torque")) ).show()

// Find the correlation coefficient between weight and hp for all cars. Display only 4 decimal points
val cor = cars.stat.<FILL_IN>("hp","weight") 
println("hp to weight : Correlation = %.4f".format(cor))


//------------------------------ Titanic dataset ------------------------------------------- #
// Download the titanic dataset from http://www.utdallas.edu/~axn112530/cs6350/dflab/titanic3_02.csv

// read in the file
import sys.process._ 
 "wget -P /tmp http://www.utdallas.edu/~axn112530/cs6350/dflab/titanic3_02.csv" !!
val filePath = "file:/tmp/titanic3_02.csv"

val passengers = spark.read.option("header","true"). option("inferSchema","true"). csv(filePath) 

// How many passengers were there on titanic?
println("Passengers has "+<FILL_IN>+" rows")
 
// Let's get the columns that we need further
val passengers1 = passengers.select(passengers("Pclass"),passengers("Survived"),passengers("Gender"),passengers("Age"),passengers("SibSp"),passengers("Parch"),passengers("Fare"))

// print the schema of passengers1
passengers1.<FILL_IN>

// Let's run some queries

// Find the gender distribution of passengers
passengers1.<FILL_IN>("Gender").<FILL_IN>().show() 

// We would like to find out the joint distribution of gender and survived columns i.e. what is the number of males and females that survived

#---------------------------------- Logistic Regression --------------------------------------------- #
#-- Download the dataset from http://www.utdallas.edu/~axn112530/cs6350/dflab/bankruptcy.data.txt ---- #
#----- Source of dataset https://archive.ics.uci.edu/ml/datasets/Qualitative_Bankruptcy -----------#

# import libraries
import org.apache.spark.mllib.evaluation.MulticlassMetrics
import org.apache.spark.mllib.classification.{LogisticRegressionWithLBFGS, LogisticRegressionModel}
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.linalg.{Vector, Vectors}

// Read data into memory in - lazy loading
val data = sc.textFile("/FileStore/tables/<Your_Path>.txt")
data.count()

// display contents of data RDD
data.collect().foreach(println)



// let's convert qualitative variables to quantitative ones
def getDoubleValue( input:String ) : Double = {
    var result:Double = 0.0
    if (input == "P")  result = 3.0 
    if (input == "A")  result = 2.0
    if (input == "N")  result = 1.0
    if (input == "NB") result = 1.0
    if (input == "B")  result = 0.0
    return result
   }

// note that the column at index 6 is the class label and the rest are features
val parsedData = data.map{line => 
    val parts = line.split(",")
    LabeledPoint(getDoubleValue(parts(6)), Vectors.dense(parts.slice(0,6).map(x => getDoubleValue(x))))
}

// display contents of parsedData RDD
parsedData.collect().foreach(println)

// another way
println(parsedData.take(10).mkString("\n"))

// for classification, you need training and test parts. You want to randomly split data into train and test
val splits = parsedData.randomSplit(Array(0.6, 0.4), seed = 11L)
val trainingData = splits(0)
val testData = splits(1)

// check the data
trainingData.take(10).foreach(println)

// train the model
val model = new LogisticRegressionWithLBFGS().setNumClasses(2).run(trainingData)


val labelAndPreds = testData.map { point =>
  val prediction = model.predict(point.features)
  (point.label, prediction)
}

val trainErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble / testData.count
println("Training Error = " + trainErr)

		

//----------------------------- Dataframe way to do it ---------------------------------------------#
// Download the car mileage dataset from http://www.utdallas.edu/~axn112530/cs6350/dflab/car-milage.csv
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.corr
import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.ml.evaluation.RegressionEvaluator

	val filePath = "/FileStore/tables/<Your_Path>/car_milage-6f50d.csv"
	val cars = spark.read.option("header","true"). option("inferSchema","true").csv(filePath)

	val cars1 = cars.na.drop() 
	val assembler = new VectorAssembler()
    assembler.setInputCols(Array("displacement","hp","torque","CRatio","RARatio","CarbBarrells","NoOfSpeed","length","width","weight","automatic"))
    assembler.setOutputCol("features")
    val cars2 = assembler.transform(cars1)
    cars2.show(40)
    //
    // Split into training & test
    //
    val train = cars2.filter(cars1("weight") <= 4000)
    val test = cars2.filter(cars1("weight") > 4000)
    test.show()
    println("Train = "+train.count()+" Test = "+test.count())
		val algLR = new LinearRegression()
		algLR.setMaxIter(100)
		algLR.setRegParam(0.3)
		algLR.setElasticNetParam(0.8)
		algLR.setLabelCol("mpg")
		//
		val mdlLR = algLR.fit(train)
		//
		println(s"Coefficients: ${mdlLR.coefficients} Intercept: ${mdlLR.intercept}")
		val trSummary = mdlLR.summary
    println(s"numIterations: ${trSummary.totalIterations}")
    println(s"Iteration Summary History: ${trSummary.objectiveHistory.toList}")
    trSummary.residuals.show()
    println(s"RMSE: ${trSummary.rootMeanSquaredError}")
    println(s"r2: ${trSummary.r2}")
    //
		// Now let us use the model to predict our test set
		//
    val predictions = mdlLR.transform(test)
    predictions.show()
    // Calculate RMSE & MSE
    val evaluator = new RegressionEvaluator()
		evaluator.setLabelCol("mpg")
		val rmse = evaluator.evaluate(predictions)
		println("Root Mean Squared Error = "+"%6.3f".format(rmse))
		//
		evaluator.setMetricName("mse")
		val mse = evaluator.evaluate(predictions)
		println("Mean Squared Error = "+"%6.3f".format(mse))
    //
    println("** That's All Folks **")		

//---------------------------- Create a logistic regression model using pipelines -------------------------------------#

import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.feature.{HashingTF, Tokenizer}
import org.apache.spark.ml.linalg.Vector
import org.apache.spark.sql.Row

// Prepare training documents from a list of (id, text, label) tuples.
val training = spark.createDataFrame(Seq(
  (0L, "a b c d e spark", 1.0),
  (1L, "b d", 0.0),
  (2L, "spark f g h", 1.0),
  (3L, "hadoop mapreduce", 0.0)
)).toDF("id", "text", "label")

// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.
val tokenizer = new Tokenizer()
  .setInputCol("text")
  .setOutputCol("words")
val hashingTF = new HashingTF()
  .setNumFeatures(1000)
  .setInputCol(tokenizer.getOutputCol)
  .setOutputCol("features")
val lr = new LogisticRegression()
  .setMaxIter(10)
  .setRegParam(0.001)
val pipeline = new Pipeline()
  .setStages(Array(tokenizer, hashingTF, lr))

// Fit the pipeline to training documents.
val model = pipeline.fit(training)

// Now we can optionally save the fitted pipeline to disk
model.write.overwrite().save("/tmp/spark-logistic-regression-model")

// We can also save this unfit pipeline to disk
pipeline.write.overwrite().save("/tmp/unfit-lr-model")

// And load it back in during production
val sameModel = PipelineModel.load("/tmp/spark-logistic-regression-model")

// Prepare test documents, which are unlabeled (id, text) tuples.
val test = spark.createDataFrame(Seq(
  (4L, "spark i j k"),
  (5L, "l m n"),
  (6L, "spark hadoop spark"),
  (7L, "apache hadoop")
)).toDF("id", "text")

// Make predictions on test documents.
model.transform(test)
  .select("id", "text", "probability", "prediction")
  .collect()
  .foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) =>
    println(s"($id, $text) --> prob=$prob, prediction=$prediction")
  }
  
//-------------------------------- Decision Tree Example --------------------------------------------------#  
 
 import org.apache.spark.sql.types.DataType
 import org.apache.spark.sql.SparkSession
 import org.apache.spark.sql.functions.corr
 import org.apache.spark.ml.regression.LinearRegression
 import org.apache.spark.ml.linalg.Vectors
 import org.apache.spark.ml.evaluation.RegressionEvaluator
 import org.apache.spark.ml.feature.{VectorAssembler,StringIndexer}
 import org.apache.spark.ml.classification.DecisionTreeClassifier
 import org.apache.spark.sql.types.DoubleType
 import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator


 val passengers1 = passengers.select(passengers("Pclass"),passengers("Survived").cast(DoubleType).as("Survived"),passengers("Gender"),passengers("Age"),passengers("SibSp"),passengers("Parch"),passengers("Fare"))
    passengers1.show(5)
        //
        // VectorAssembler does not support the StringType type. So convert Gender to numeric
        //
    val indexer = new StringIndexer()
    indexer.setInputCol("Gender")
    indexer.setOutputCol("GenderCat")
    val passengers2 = indexer.fit(passengers1).transform(passengers1)
    passengers2.show(5)
        //
    val passengers3 = passengers2.na.drop()
    println("Orig = "+passengers2.count()+" Final = "+ passengers3.count() + "Dropped = "+ (passengers2.count() - passengers3.count()))
        //
    val assembler = new VectorAssembler()
    assembler.setInputCols(Array("Pclass","GenderCat","Age","SibSp","Parch","Fare"))
    assembler.setOutputCol("features")
    val passengers4 = assembler.transform(passengers3)
    passengers4.show(5)
	
	val Array(train, test) = passengers4.randomSplit(Array(0.9, 0.1))
	println("Train = "+train.count()+" Test = "+test.count())
	
	val algTree = new DecisionTreeClassifier()
	algTree.setLabelCol("Survived")
	algTree.setImpurity("gini") // could be "entropy"
	algTree.setMaxBins(32)
	algTree.setMaxDepth(5)
	       //
	val mdlTree = algTree.fit(train)
	println("The tree has %d nodes.".format(mdlTree.numNodes))
	println(mdlTree.toDebugString)
	println(mdlTree.toString)
	println(mdlTree.featureImportances)
	
	val predictions = mdlTree.transform(test)
	predictions.show(5)
	
	// model evaluation
	val evaluator = new MulticlassClassificationEvaluator()
	evaluator.setLabelCol("Survived")
	evaluator.setMetricName("accuracy") // could be f1, "weightedPrecision" or "weightedRecall"
	       //
	val startTime = System.nanoTime()
	val accuracy = evaluator.evaluate(predictions)
	println("Test Accuracy = %.2f%%".format(accuracy*100))
	       //
	val elapsedTime = (System.nanoTime() - startTime) / 1e9
	println("Elapsed time: %.2fseconds".format(elapsedTime))
	
	
 
//-------------------------------- Hyperparameter tuning example --------------------------------------------------#
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
import org.apache.spark.ml.feature.{HashingTF, Tokenizer}
import org.apache.spark.ml.linalg.Vector
import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}
import org.apache.spark.sql.Row

// Prepare training data from a list of (id, text, label) tuples.
val training = spark.createDataFrame(Seq(
  (0L, "a b c d e spark", 1.0),
  (1L, "b d", 0.0),
  (2L, "spark f g h", 1.0),
  (3L, "hadoop mapreduce", 0.0),
  (4L, "b spark who", 1.0),
  (5L, "g d a y", 0.0),
  (6L, "spark fly", 1.0),
  (7L, "was mapreduce", 0.0),
  (8L, "e spark program", 1.0),
  (9L, "a e c l", 0.0),
  (10L, "spark compile", 1.0),
  (11L, "hadoop software", 0.0)
)).toDF("id", "text", "label")

// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.
val tokenizer = new Tokenizer()
  .setInputCol("text")
  .setOutputCol("words")
val hashingTF = new HashingTF()
  .setInputCol(tokenizer.getOutputCol)
  .setOutputCol("features")
val lr = new LogisticRegression()
  .setMaxIter(10)
val pipeline = new Pipeline()
  .setStages(Array(tokenizer, hashingTF, lr))

// We use a ParamGridBuilder to construct a grid of parameters to search over.
// With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,
// this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.
val paramGrid = new ParamGridBuilder()
  .addGrid(hashingTF.numFeatures, Array(10, 100, 1000))
  .addGrid(lr.regParam, Array(0.1, 0.01))
  .build()

// We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.
// This will allow us to jointly choose parameters for all Pipeline stages.
// A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.
// Note that the evaluator here is a BinaryClassificationEvaluator and its default metric
// is areaUnderROC.
val cv = new CrossValidator()
  .setEstimator(pipeline)
  .setEvaluator(new BinaryClassificationEvaluator)
  .setEstimatorParamMaps(paramGrid)
  .setNumFolds(2)  // Use 3+ in practice

// Run cross-validation, and choose the best set of parameters.
val cvModel = cv.fit(training)

// Prepare test documents, which are unlabeled (id, text) tuples.
val test = spark.createDataFrame(Seq(
  (4L, "spark i j k"),
  (5L, "l m n"),
  (6L, "mapreduce spark"),
  (7L, "apache hadoop")
)).toDF("id", "text")

// Make predictions on test documents. cvModel uses the best model found (lrModel).
cvModel.transform(test)
  .select("id", "text", "probability", "prediction")
  .collect()
  .foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) =>
    println(s"($id, $text) --> prob=$prob, prediction=$prediction")
  }

------------------ Collaborative Filtering ------------------

import org.apache.spark.ml.evaluation.RegressionEvaluator
import org.apache.spark.ml.recommendation.ALS

import sys.process._ 
 "wget -P /tmp http://www.utdallas.edu/~axn112530/cs6350/data/sample_movielens_ratings.txt" !!

case class Rating(userId: Int, movieId: Int, rating: Float, timestamp: Long)
def parseRating(str: String): Rating = {
  val fields = str.split("::")
  assert(fields.size == 4)
  Rating(fields(0).toInt, fields(1).toInt, fields(2).toFloat, fields(3).toLong)
}

val ratings = spark.read.textFile("file:/tmp/sample_movielens_ratings.txt")
  .map(parseRating)
  .toDF()
val Array(training, test) = ratings.randomSplit(Array(0.8, 0.2))

// Build the recommendation model using ALS on the training data
val als = new ALS()
  .setMaxIter(5)
  .setRegParam(0.01)
  .setUserCol("userId")
  .setItemCol("movieId")
  .setRatingCol("rating")
val model = als.fit(training)

// Evaluate the model by computing the RMSE on the test data
val predictions = model.transform(test)

val evaluator = new RegressionEvaluator()
  .setMetricName("rmse")
  .setLabelCol("rating")
  .setPredictionCol("prediction")
val rmse = evaluator.evaluate(predictions)
println(s"Root-mean-square error = $rmse")

//------------------------------ Recommender System ------------------------------------------- #
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.Row
import org.apache.spark.sql.functions.{split,pow,isnan}
import org.apache.spark.mllib.recommendation.Rating
import org.apache.spark.ml.recommendation.ALS
import org.apache.spark.ml.evaluation.RegressionEvaluator
import org.apache.log4j.{Level, Logger}

import sys.process._ 
 "wget -P /tmp http://www.utdallas.edu/~axn112530/cs6350/data/movielens/movies.dat" !!
import sys.process._ 
"wget -P /tmp http://www.utdallas.edu/~axn112530/cs6350/data/movielens/ratings.dat" !!
import sys.process._ 
"wget -P /tmp http://www.utdallas.edu/~axn112530/cs6350/data/movielens/users.dat" !!

val filePathMovies = "file:/tmp/movies.dat"
val filePathRatings = "file:/tmp/ratings.dat"
val filePathUsers = "file:/tmp/users.dat"

val movies = spark.read.text(filePathMovies)
val ratings = spark.read.text(filePathRatings)
val users = spark.read.text(filePathUsers)
println("Got %d ratings from %d users on %d movies.".format(ratings.count(), users.count(), movies.count()))

	movies.show(5,truncate=false)
	ratings.show(5,truncate=false)
	users.show(5,truncate=false)

	def parseRating(row:Row) : Rating = {
		  val aList = row.getList[String](0)
		  Rating(aList.get(0).toInt,aList.get(1).toInt,aList.get(2).toDouble) //.getInt(0), row.getInt(1), row.getDouble(2))
	}


	val ratings1 = ratings.select(split(ratings("value"),"::")).as("values")
    ratings1.show(5)
    val ratings2 = ratings1.rdd.map(row => parseRating(row))
    ratings2.take(3).foreach(println)
    //
    val ratings3 = spark.createDataFrame(ratings2)
    ratings3.show(5)
	
	

	val ratings1 = ratings.select(split(ratings("value"),"::")).as("values")
	ratings1.show(5)
	val ratings2 = ratings1.rdd.map(row => parseRating(row))
	ratings2.take(3).foreach(println)
	//
	val ratings3 = spark.createDataFrame(ratings2)
	ratings3.show(5)
	
	val Array(train, test) = ratings3.randomSplit(Array(0.8, 0.2))
	println("Train = "+train.count()+" Test = "+test.count())
	
	val algALS = new ALS()
	algALS.setItemCol("product") // Otherwise will get exception "Field "item" does not exist"
	algALS.setRank(12)
	algALS.setRegParam(0.1) 
	algALS.setMaxIter(20)
	val mdlReco = algALS.fit(train)
	
	val predictions = mdlReco.transform(test)
	predictions.show(5)
	predictions.printSchema()
	
	val pred = predictions.na.drop()
	println("Orig = "+predictions.count()+" Final = "+ pred.count() + " Dropped = "+ (predictions.count() - pred.count()))
	// Calculate RMSE & MSE
	val evaluator = new RegressionEvaluator()
	evaluator.setLabelCol("rating")
	var rmse = evaluator.evaluate(pred)
	println("Root Mean Squared Error = "+"%.3f".format(rmse))
	//
	evaluator.setMetricName("mse")
	var mse = evaluator.evaluate(pred)
	println("Mean Squared Error = "+"%.3f".format(mse))
	mse = pred.rdd.map(r => rowSqDiff(r)).reduce(_+_) / predictions.count().toDouble
	println("Mean Squared Error (Calculated) = "+"%.3f".format(mse))
	

//------------------------------ Frequent Pattern Growth ------------------------------------------- #
import org.apache.spark.mllib.fpm.FPGrowth
import org.apache.spark.rdd.RDD

val data = sc.textFile("/databricks-datasets/samples/data/mllib/sample_fpgrowth.txt")

val transactions: RDD[Array[String]] = data.map(s => s.trim.split(' '))

val fpg = new FPGrowth()
  .setMinSupport(0.2)
  .setNumPartitions(10)
val model = fpg.run(transactions)

model.freqItemsets.collect().foreach { itemset =>
  println(itemset.items.mkString("[", ",", "]") + ", " + itemset.freq)
}

val minConfidence = 0.8
model.generateAssociationRules(minConfidence).collect().foreach { rule =>
  println(
    rule.antecedent.mkString("[", ",", "]")
      + " => " + rule.consequent .mkString("[", ",", "]")
      + ", " + rule.confidence)
}

---------- Dimensionality Reduction --------------
import org.apache.spark.mllib.feature.PCA
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.rdd.RDD

val data: RDD[LabeledPoint] = sc.parallelize(Seq(
  new LabeledPoint(0, Vectors.dense(1, 0, 0, 0, 1)),
  new LabeledPoint(1, Vectors.dense(1, 1, 0, 1, 0)),
  new LabeledPoint(1, Vectors.dense(1, 1, 0, 0, 0)),
  new LabeledPoint(0, Vectors.dense(1, 0, 0, 0, 0)),
  new LabeledPoint(1, Vectors.dense(1, 1, 0, 0, 0))))

// Compute the top 5 principal components.
val pca = new PCA(5).fit(data.map(_.features))

// Project vectors to the linear space spanned by the top 5 principal
// components, keeping the label
val projected = data.map(p => p.copy(features = pca.transform(p.features)))

---------- K-Means ------------------------------
import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}
import org.apache.spark.mllib.linalg.Vectors

// Load and parse the data
val data = sc.textFile("/databricks-datasets/samples/data/mllib/kmeans_data.txt")
val parsedData = data.map(s => Vectors.dense(s.split(' ').map(_.toDouble))).cache()

// Cluster the data into two classes using KMeans
val numClusters = 2
val numIterations = 20
val clusters = KMeans.train(parsedData, numClusters, numIterations)

// Evaluate clustering by computing Within Set Sum of Squared Errors
val WSSSE = clusters.computeCost(parsedData)
println("Within Set Sum of Squared Errors = " + WSSSE)

# ---------- k-means on very large data ------------------------------

// Download data from http://www.utdallas.edu/~axn112530/cs6350/data/retail-data/

import sys.process._ 
"wget -r --no-parent --convert-links -nH -nd -P /FileStore/retail-data http://www.utdallas.edu/~axn112530/cs6350/data/retail-data/" !!

val staticDataFrame = spark.read.format("csv")
   .option("header", "true")
   .option("inferSchema", "true")
   .load("file:/FileStore/retail-data/*.csv")
   
import org.apache.spark.sql.functions.date_format
val preppedDataFrame = staticDataFrame
   .na.fill(0)
   .withColumn("day_of_week", date_format($"InvoiceDate", "EEEE"))
   .coalesce(5)
   
   
val trainDataFrame = preppedDataFrame
   .where("InvoiceDate < '2011-07-01'")
val testDataFrame = preppedDataFrame
   .where("InvoiceDate >= '2011-07-01'")
   
trainDataFrame.count()
testDataFrame.count()

import org.apache.spark.ml.feature.StringIndexer
val indexer = new StringIndexer()
   .setInputCol("day_of_week")
   .setOutputCol("day_of_week_index")
   
import org.apache.spark.ml.feature.OneHotEncoder
val encoder = new OneHotEncoder()
   .setInputCol("day_of_week_index")
   .setOutputCol("day_of_week_encoded")

import org.apache.spark.ml.feature.VectorAssembler
val vectorAssembler = new VectorAssembler()
   .setInputCols(Array("UnitPrice", "Quantity", "day_of_week_encoded"))
   .setOutputCol("features")
   
import org.apache.spark.ml.Pipeline
val transformationPipeline = new Pipeline()
   .setStages(Array(indexer, encoder, vectorAssembler))
   

val fittedPipeline = transformationPipeline.fit(trainDataFrame)

val transformedTraining = fittedPipeline.transform(trainDataFrame)

transformedTraining.cache()

import org.apache.spark.ml.clustering.KMeans
val kmeans = new KMeans()
   .setK(20)
   .setSeed(1L)

val kmModel = kmeans.fit(transformedTraining)

kmModel.computeCost(transformedTraining)

val transformedTest =  fittedPipeline.transform(testDataFrame)

display(kmModel.summary.predictions)

display(kmModel.transform(transformedTest))